# Tokenizers

Tokenizers are the first stage in the "LLM Pipeline." They convert text into "tokens" (a unit that can be processed by a transformer). Project faciliated by BU AI society. Small scale tokenizer trained on Shakespear(feel free to download and try your own dataset) Capable of holding up to 800 unique tokens


## Prereqs

- just python past 3.8


## Installation

need to the regex package (python's built in `re` will not be good enough). You can install it with `pip`:
```bash
pip install regex
# or
pip3 install regex
```




